{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# ƒê·ªí √ÅN M√îN H·ªåC: DEEP LEARNING\n\n# **D·ª∞ B√ÅO NHU C·∫¶U B√ÅN H√ÄNG S·ª¨ D·ª§NG C√ÅC M√î H√åNH DEEP LEARNING**\n## (Demand Forecasting using Deep Learning Models)\n\n---\n\n**Sinh vi√™n th·ª±c hi·ªán:** [H·ªç v√† t√™n]\n\n**MSSV:** [M√£ s·ªë sinh vi√™n]\n\n**Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n:** [T√™n gi·∫£ng vi√™n]\n\n**Tr∆∞·ªùng:** ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin - ƒêHQG TP.HCM (UIT)\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## M·ª§C L·ª§C\n\n1. [Gi·ªõi thi·ªáu b√†i to√°n](#1-gi·ªõi-thi·ªáu-b√†i-to√°n)\n2. [C∆° s·ªü l√Ω thuy·∫øt](#2-c∆°-s·ªü-l√Ω-thuy·∫øt)\n3. [M√¥ t·∫£ d·ªØ li·ªáu](#3-m√¥-t·∫£-d·ªØ-li·ªáu)\n4. [Ph√¢n t√≠ch d·ªØ li·ªáu kh√°m ph√° (EDA)](#4-ph√¢n-t√≠ch-d·ªØ-li·ªáu-kh√°m-ph√°-eda)\n5. [Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu](#5-ti·ªÅn-x·ª≠-l√Ω-d·ªØ-li·ªáu)\n6. [X√¢y d·ª±ng m√¥ h√¨nh](#6-x√¢y-d·ª±ng-m√¥-h√¨nh)\n7. [ƒê√°nh gi√° v√† so s√°nh](#7-ƒë√°nh-gi√°-v√†-so-s√°nh)\n8. [K·∫øt lu·∫≠n](#8-k·∫øt-lu·∫≠n)\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 1. GI·ªöI THI·ªÜU B√ÄI TO√ÅN\n\n### 1.1. ƒê·∫∑t v·∫•n ƒë·ªÅ\n\n**D·ª± b√°o nhu c·∫ßu (Demand Forecasting)** l√† m·ªôt trong nh·ªØng b√†i to√°n quan tr·ªçng nh·∫•t trong lƒ©nh v·ª±c kinh doanh v√† chu·ªói cung ·ª©ng. Vi·ªác d·ª± ƒëo√°n ch√≠nh x√°c nhu c·∫ßu s·∫£n ph·∫©m gi√∫p doanh nghi·ªáp:\n\n- **T·ªëi ∆∞u h√≥a t·ªìn kho:** Tr√°nh t√¨nh tr·∫°ng thi·∫øu h√†ng ho·∫∑c t·ªìn kho qu√° m·ª©c\n- **L·∫≠p k·∫ø ho·∫°ch s·∫£n xu·∫•t:** ƒêi·ªÅu ch·ªânh nƒÉng l·ª±c s·∫£n xu·∫•t ph√π h·ª£p v·ªõi nhu c·∫ßu\n- **Qu·∫£n l√Ω ngu·ªìn l·ª±c:** Ph√¢n b·ªï nh√¢n s·ª± v√† t√†i nguy√™n hi·ªáu qu·∫£\n- **TƒÉng l·ª£i nhu·∫≠n:** Gi·∫£m chi ph√≠ l∆∞u kho v√† tƒÉng doanh thu\n\n### 1.2. M·ª•c ti√™u ƒë·ªì √°n\n\nƒê·ªì √°n n√†y nh·∫±m m·ª•c ƒë√≠ch:\n\n1. **Kh√°m ph√° v√† ph√¢n t√≠ch** d·ªØ li·ªáu b√°n h√†ng theo chu·ªói th·ªùi gian\n2. **X√¢y d·ª±ng v√† so s√°nh** c√°c m√¥ h√¨nh Deep Learning cho b√†i to√°n d·ª± b√°o:\n   - Multi-Layer Perceptron (MLP)\n   - Convolutional Neural Network (CNN)\n   - Long Short-Term Memory (LSTM)\n   - Hybrid CNN-LSTM\n3. **ƒê√°nh gi√° hi·ªáu su·∫•t** c·ªßa t·ª´ng m√¥ h√¨nh v√† ƒë∆∞a ra khuy·∫øn ngh·ªã\n\n### 1.3. Ph·∫°m vi nghi√™n c·ª©u\n\n- **D·ªØ li·ªáu:** T·∫≠p d·ªØ li·ªáu \"Store Item Demand Forecasting\" t·ª´ Kaggle\n- **Horizon d·ª± b√°o:** 90 ng√†y\n- **Ph∆∞∆°ng ph√°p:** Supervised Learning v·ªõi Time Series Features\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 2. C∆† S·ªû L√ù THUY·∫æT\n\n### 2.1. D·ªØ li·ªáu chu·ªói th·ªùi gian (Time Series)\n\n**Chu·ªói th·ªùi gian** l√† m·ªôt chu·ªói c√°c ƒëi·ªÉm d·ªØ li·ªáu ƒë∆∞·ª£c ƒëo l∆∞·ªùng theo th·ªùi gian v·ªõi kho·∫£ng c√°ch ƒë·ªÅu nhau. C√°c ƒë·∫∑c ƒëi·ªÉm quan tr·ªçng c·ªßa chu·ªói th·ªùi gian bao g·ªìm:\n\n- **Trend (Xu h∆∞·ªõng):** H∆∞·ªõng ƒëi d√†i h·∫°n c·ªßa d·ªØ li·ªáu (tƒÉng, gi·∫£m, ho·∫∑c ·ªïn ƒë·ªãnh)\n- **Seasonality (T√≠nh m√πa v·ª•):** C√°c pattern l·∫∑p l·∫°i theo chu k·ª≥ c·ªë ƒë·ªãnh (ng√†y, tu·∫ßn, th√°ng, nƒÉm)\n- **Cyclical (Chu k·ª≥):** Bi·∫øn ƒë·ªông kh√¥ng c√≥ chu k·ª≥ c·ªë ƒë·ªãnh\n- **Noise (Nhi·ªÖu):** Bi·∫øn ƒë·ªông ng·∫´u nhi√™n kh√¥ng th·ªÉ d·ª± ƒëo√°n\n\n### 2.2. Chuy·ªÉn ƒë·ªïi Time Series sang Supervised Learning\n\nƒê·ªÉ √°p d·ª•ng c√°c m√¥ h√¨nh Machine Learning, ta c·∫ßn chuy·ªÉn ƒë·ªïi b√†i to√°n time series th√†nh b√†i to√°n supervised learning b·∫±ng ph∆∞∆°ng ph√°p **Sliding Window**:\n\n```\nInput (X):  [t-n, t-n+1, ..., t-1, t]   ‚Üí   Output (y): [t+lag]\n```\n\nV√≠ d·ª• v·ªõi window=3 v√† lag=1:\n```\nX: [sales_day1, sales_day2, sales_day3]  ‚Üí  y: sales_day4\nX: [sales_day2, sales_day3, sales_day4]  ‚Üí  y: sales_day5\n...\n```\n\n### 2.3. C√°c m√¥ h√¨nh Deep Learning\n\n#### 2.3.1. Multi-Layer Perceptron (MLP)\n\nMLP l√† m·∫°ng neural c∆° b·∫£n g·ªìm nhi·ªÅu l·ªõp fully-connected. V·ªõi time series:\n- **∆Øu ƒëi·ªÉm:** ƒê∆°n gi·∫£n, train nhanh\n- **Nh∆∞·ª£c ƒëi·ªÉm:** Kh√¥ng capture ƒë∆∞·ª£c sequential patterns, coi c√°c timesteps l√† ƒë·ªôc l·∫≠p\n\n#### 2.3.2. Convolutional Neural Network (CNN)\n\nCNN s·ª≠ d·ª•ng c√°c b·ªô l·ªçc (filters) tr∆∞·ª£t qua d·ªØ li·ªáu ƒë·ªÉ tr√≠ch xu·∫•t features:\n- **∆Øu ƒëi·ªÉm:** Ph√°t hi·ªán local patterns, t√≠nh to√°n song song\n- **Nh∆∞·ª£c ƒëi·ªÉm:** Kh√≥ capture long-range dependencies\n\n#### 2.3.3. Long Short-Term Memory (LSTM)\n\nLSTM l√† bi·∫øn th·ªÉ c·ªßa RNN, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ h·ªçc c√°c dependencies d√†i h·∫°n:\n- **∆Øu ƒëi·ªÉm:** X·ª≠ l√Ω sequential data t·ªët, nh·ªõ th√¥ng tin d√†i h·∫°n\n- **Nh∆∞·ª£c ƒëi·ªÉm:** Train ch·∫≠m, nhi·ªÅu parameters\n\n#### 2.3.4. Hybrid CNN-LSTM\n\nK·∫øt h·ª£p CNN v√† LSTM:\n- CNN: Tr√≠ch xu·∫•t local features t·ª´ subsequences\n- LSTM: H·ªçc temporal dependencies gi·ªØa c√°c subsequences\n\n### 2.4. Metrics ƒë√°nh gi√°\n\n**Root Mean Squared Error (RMSE):**\n\n$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n\nRMSE ƒëo l∆∞·ªùng sai s·ªë trung b√¨nh gi·ªØa gi√° tr·ªã th·ª±c v√† gi√° tr·ªã d·ª± ƒëo√°n, v·ªõi ƒë∆°n v·ªã gi·ªëng d·ªØ li·ªáu g·ªëc.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3. M√î T·∫¢ D·ªÆ LI·ªÜU V√Ä THI·∫æT L·∫¨P M√îI TR∆Ø·ªúNG\n\n### 3.1. Import th∆∞ vi·ªán\n\nC√°c th∆∞ vi·ªán ƒë∆∞·ª£c s·ª≠ d·ª•ng trong ƒë·ªì √°n:\n- **NumPy, Pandas:** X·ª≠ l√Ω d·ªØ li·ªáu\n- **Matplotlib, Plotly:** Tr·ª±c quan h√≥a\n- **TensorFlow/Keras:** X√¢y d·ª±ng m√¥ h√¨nh Deep Learning\n- **Scikit-learn:** Ti·ªÅn x·ª≠ l√Ω v√† ƒë√°nh gi√°",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import warnings\nimport os\n\n# === C·∫•u h√¨nh TensorFlow ch·∫°y tr√™n CPU ===\n# Ph·∫£i ƒë·∫∑t TR∆Ø·ªöC khi import tensorflow ƒë·ªÉ tr√°nh l·ªói v·ªõi Metal GPU tr√™n Mac\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Th∆∞ vi·ªán x·ª≠ l√Ω d·ªØ li·ªáu\nimport numpy as np\nimport pandas as pd\n\n# Th∆∞ vi·ªán tr·ª±c quan h√≥a\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\n# TensorFlow v√† Keras\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Conv1D, MaxPooling1D, Dense, LSTM, \n    RepeatVector, TimeDistributed, Flatten\n)\n\n# Scikit-learn\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# C·∫•u h√¨nh hi·ªÉn th·ªã\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\n# ƒê·∫∑t random seed ƒë·ªÉ ƒë·∫£m b·∫£o reproducibility\nRANDOM_SEED = 1\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\nprint(f\"‚úì TensorFlow version: {tf.__version__}\")\nprint(f\"‚úì NumPy version: {np.__version__}\")\nprint(f\"‚úì Pandas version: {pd.__version__}\")\nprint(f\"‚úì Ch·∫ø ƒë·ªô ch·∫°y: CPU only\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2. T·∫£i d·ªØ li·ªáu\n\n**Ngu·ªìn d·ªØ li·ªáu:** [Kaggle - Store Item Demand Forecasting Challenge](https://www.kaggle.com/c/demand-forecasting-kernels-only)\n\n**M√¥ t·∫£ dataset:**\n- D·ªØ li·ªáu b√°n h√†ng c·ªßa 10 c·ª≠a h√†ng v·ªõi 50 s·∫£n ph·∫©m\n- Kho·∫£ng th·ªùi gian: 5 nƒÉm (2013-2017)\n- T·ªïng s·ªë records: ~913,000 d√≤ng\n\n**C√°c c·ªôt d·ªØ li·ªáu:**\n| C·ªôt | M√¥ t·∫£ |\n|-----|-------|\n| date | Ng√†y b√°n h√†ng |\n| store | M√£ c·ª≠a h√†ng (1-10) |\n| item | M√£ s·∫£n ph·∫©m (1-50) |\n| sales | S·ªë l∆∞·ª£ng b√°n |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu\n# Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n n√†y n·∫øu d·ªØ li·ªáu ·ªü v·ªã tr√≠ kh√°c\nDATA_PATH = './data/'\n\n# ƒê·ªçc d·ªØ li·ªáu training v√† test\ntrain = pd.read_csv(DATA_PATH + 'train.csv', parse_dates=['date'])\ntest = pd.read_csv(DATA_PATH + 'test.csv', parse_dates=['date'])\n\nprint(\"=\" * 50)\nprint(\"TH√îNG TIN T·ªîNG QUAN V·ªÄ D·ªÆ LI·ªÜU\")\nprint(\"=\" * 50)\nprint(f\"\\nüìä K√≠ch th∆∞·ªõc t·∫≠p TRAIN: {train.shape[0]:,} d√≤ng √ó {train.shape[1]} c·ªôt\")\nprint(f\"üìä K√≠ch th∆∞·ªõc t·∫≠p TEST: {test.shape[0]:,} d√≤ng √ó {test.shape[1]} c·ªôt\")\nprint(f\"\\nüìÖ S·ªë c·ª≠a h√†ng: {train['store'].nunique()}\")\nprint(f\"üì¶ S·ªë s·∫£n ph·∫©m: {train['item'].nunique()}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 3.3. Kh√°m ph√° c·∫•u tr√∫c d·ªØ li·ªáu\n\nTr∆∞·ªõc khi ti·∫øn h√†nh ph√¢n t√≠ch, c·∫ßn hi·ªÉu r√µ c·∫•u tr√∫c v√† ƒë·∫∑c ƒëi·ªÉm c·ªßa d·ªØ li·ªáu.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Xem th√¥ng tin chi ti·∫øt v·ªÅ dataset\nprint(\"\\nüìã TH√îNG TIN CHI TI·∫æT C√ÅC C·ªòT:\")\nprint(\"-\" * 40)\ntrain.info()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Xem 5 d√≤ng ƒë·∫ßu ti√™n\nprint(\"\\nüìù M·∫™U D·ªÆ LI·ªÜU (5 d√≤ng ƒë·∫ßu):\")\ntrain.head()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Th·ªëng k√™ m√¥ t·∫£\nprint(\"\\nüìà TH·ªêNG K√ä M√î T·∫¢:\")\ntrain.describe()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 3.4. X√°c ƒë·ªãnh kho·∫£ng th·ªùi gian v√† horizon d·ª± b√°o\n\nVi·ªác x√°c ƒë·ªãnh **forecast horizon** (kho·∫£ng th·ªùi gian c·∫ßn d·ª± b√°o) r·∫•t quan tr·ªçng v√¨ n√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn c√°ch x√¢y d·ª±ng features v√† ƒë√°nh gi√° m√¥ h√¨nh.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Kho·∫£ng th·ªùi gian c·ªßa t·∫≠p train\nprint(\"üìÖ KHO·∫¢NG TH·ªúI GIAN D·ªÆ LI·ªÜU\")\nprint(\"=\" * 40)\nprint(f\"Ng√†y b·∫Øt ƒë·∫ßu (Train): {train['date'].min().date()}\")\nprint(f\"Ng√†y k·∫øt th√∫c (Train): {train['date'].max().date()}\")\nprint(f\"\\nNg√†y b·∫Øt ƒë·∫ßu (Test): {test['date'].min().date()}\")\nprint(f\"Ng√†y k·∫øt th√∫c (Test): {test['date'].max().date()}\")\n\n# T√≠nh forecast horizon (lag)\nlag_size = (test['date'].max().date() - train['date'].max().date()).days\nprint(f\"\\nüéØ FORECAST HORIZON: {lag_size} ng√†y\")\nprint(f\"   ‚Üí M√¥ h√¨nh c·∫ßn d·ª± b√°o doanh s·ªë {lag_size} ng√†y trong t∆∞∆°ng lai\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 4. PH√ÇN T√çCH D·ªÆ LI·ªÜU KH√ÅM PH√Å (EDA)\n\nTrong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω tr·ª±c quan h√≥a d·ªØ li·ªáu ƒë·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ:\n- Xu h∆∞·ªõng b√°n h√†ng theo th·ªùi gian\n- Patterns theo c·ª≠a h√†ng v√† s·∫£n ph·∫©m\n- T√≠nh m√πa v·ª• (seasonality)\n\n### 4.1. T·ªïng h·ª£p d·ªØ li·ªáu theo c√°c chi·ªÅu ph√¢n t√≠ch",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# T·ªïng doanh s·ªë theo ng√†y (to√†n b·ªô c·ª≠a h√†ng & s·∫£n ph·∫©m)\ndaily_sales = train.groupby('date', as_index=False)['sales'].sum()\n\n# Doanh s·ªë theo c·ª≠a h√†ng v√† ng√†y\nstore_daily_sales = train.groupby(['store', 'date'], as_index=False)['sales'].sum()\n\n# Doanh s·ªë theo s·∫£n ph·∫©m v√† ng√†y\nitem_daily_sales = train.groupby(['item', 'date'], as_index=False)['sales'].sum()\n\nprint(\"‚úì ƒê√£ t·ªïng h·ª£p d·ªØ li·ªáu theo c√°c chi·ªÅu ph√¢n t√≠ch\")\nprint(f\"  - Daily sales: {len(daily_sales)} records\")\nprint(f\"  - Store daily sales: {len(store_daily_sales)} records\")\nprint(f\"  - Item daily sales: {len(item_daily_sales)} records\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2. Xu h∆∞·ªõng doanh s·ªë t·ªïng theo th·ªùi gian\n\nBi·ªÉu ƒë·ªì d∆∞·ªõi ƒë√¢y th·ªÉ hi·ªán t·ªïng doanh s·ªë b√°n h√†ng c·ªßa t·∫•t c·∫£ c·ª≠a h√†ng theo th·ªùi gian. Qua ƒë√≥ c√≥ th·ªÉ quan s√°t:\n- **Trend:** Doanh s·ªë c√≥ xu h∆∞·ªõng tƒÉng/gi·∫£m kh√¥ng?\n- **Seasonality:** C√≥ pattern l·∫∑p l·∫°i theo nƒÉm kh√¥ng?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Bi·ªÉu ƒë·ªì t·ªïng doanh s·ªë theo ng√†y\ndaily_sales_sc = go.Scatter(\n    x=daily_sales['date'], \n    y=daily_sales['sales'],\n    mode='lines',\n    name='Daily Sales',\n    line=dict(color='#2E86AB', width=1)\n)\n\nlayout = go.Layout(\n    title=dict(text='<b>T·ªïng doanh s·ªë b√°n h√†ng theo ng√†y (2013-2017)</b>', x=0.5),\n    xaxis=dict(title='Ng√†y', showgrid=True, gridcolor='lightgray'),\n    yaxis=dict(title='T·ªïng s·ªë l∆∞·ª£ng b√°n', showgrid=True, gridcolor='lightgray'),\n    template='plotly_white',\n    hovermode='x unified'\n)\n\nfig = go.Figure(data=[daily_sales_sc], layout=layout)\niplot(fig)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "**üìä Nh·∫≠n x√©t:**\n- Doanh s·ªë c√≥ **xu h∆∞·ªõng tƒÉng d·∫ßn** qua c√°c nƒÉm (trend tƒÉng)\n- Xu·∫•t hi·ªán **t√≠nh m√πa v·ª• r√µ r√†ng** v·ªõi chu k·ª≥ h√†ng nƒÉm (peaks v√†o gi·ªØa nƒÉm, th·∫•p v√†o ƒë·∫ßu nƒÉm)\n- Bi√™n ƒë·ªô dao ƒë·ªông c≈©ng tƒÉng theo th·ªùi gian (heteroscedasticity)\n\n### 4.3. So s√°nh doanh s·ªë gi·ªØa c√°c c·ª≠a h√†ng\n\nPh√¢n t√≠ch hi·ªáu su·∫•t b√°n h√†ng c·ªßa t·ª´ng c·ª≠a h√†ng ƒë·ªÉ xem c√≥ s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ kh√¥ng.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Bi·ªÉu ƒë·ªì doanh s·ªë theo t·ª´ng c·ª≠a h√†ng\nstore_daily_sales_sc = []\ncolors = ['#E63946', '#F4A261', '#2A9D8F', '#264653', '#E9C46A',\n          '#023E8A', '#0077B6', '#00B4D8', '#90BE6D', '#F94144']\n\nfor i, store in enumerate(sorted(store_daily_sales['store'].unique())):\n    store_data = store_daily_sales[store_daily_sales['store'] == store]\n    store_daily_sales_sc.append(\n        go.Scatter(\n            x=store_data['date'], \n            y=store_data['sales'], \n            name=f'C·ª≠a h√†ng {store}',\n            line=dict(width=1, color=colors[i % len(colors)])\n        )\n    )\n\nlayout = go.Layout(\n    title=dict(text='<b>Doanh s·ªë b√°n h√†ng theo t·ª´ng c·ª≠a h√†ng</b>', x=0.5),\n    xaxis=dict(title='Ng√†y'),\n    yaxis=dict(title='S·ªë l∆∞·ª£ng b√°n'),\n    template='plotly_white',\n    legend=dict(orientation='h', yanchor='bottom', y=1.02)\n)\n\nfig = go.Figure(data=store_daily_sales_sc, layout=layout)\niplot(fig)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "**üìä Nh·∫≠n x√©t:**\n- C√°c c·ª≠a h√†ng c√≥ **pattern t∆∞∆°ng t·ª± nhau** (c√πng tƒÉng/gi·∫£m theo m√πa)\n- **C·ª≠a h√†ng 2** c√≥ doanh s·ªë cao nh·∫•t, **c·ª≠a h√†ng 5, 7** c√≥ doanh s·ªë th·∫•p h∆°n\n- ƒêi·ªÅu n√†y cho th·∫•y seasonality l√† y·∫øu t·ªë chung, nh∆∞ng scale kh√°c nhau gi·ªØa c√°c c·ª≠a h√†ng\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5. TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU\n\n### 5.1. L·ªçc d·ªØ li·ªáu\n\nƒê·ªÉ gi·∫£m th·ªùi gian training v√† t·∫≠p trung v√†o d·ªØ li·ªáu g·∫ßn ƒë√¢y nh·∫•t (relevant h∆°n cho d·ª± b√°o), ch√∫ng ta ch·ªâ s·ª≠ d·ª•ng **d·ªØ li·ªáu nƒÉm 2017** (nƒÉm cu·ªëi c√πng trong dataset).\n\n**L√Ω do:**\n- D·ªØ li·ªáu g·∫ßn ƒë√¢y ph·∫£n √°nh t·ªët h∆°n xu h∆∞·ªõng hi·ªán t·∫°i\n- Gi·∫£m computational cost\n- Tr√°nh m√¥ h√¨nh h·ªçc t·ª´ patterns qu√° c≈© c√≥ th·ªÉ ƒë√£ thay ƒë·ªïi",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# L∆∞u k√≠ch th∆∞·ªõc ban ƒë·∫ßu\noriginal_size = len(train)\n\n# L·ªçc ch·ªâ l·∫•y d·ªØ li·ªáu t·ª´ 2017\ntrain = train[train['date'] >= '2017-01-01']\n\nprint(\"üìâ L·ªåC D·ªÆ LI·ªÜU\")\nprint(\"=\" * 40)\nprint(f\"K√≠ch th∆∞·ªõc ban ƒë·∫ßu: {original_size:,} d√≤ng\")\nprint(f\"K√≠ch th∆∞·ªõc sau l·ªçc: {len(train):,} d√≤ng\")\nprint(f\"Gi·∫£m: {(1 - len(train)/original_size)*100:.1f}%\")\nprint(f\"\\nKho·∫£ng th·ªùi gian: {train['date'].min().date()} ‚Üí {train['date'].max().date()}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 5.2. T√°i c·∫•u tr√∫c d·ªØ li·ªáu\n\nS·∫Øp x·∫øp v√† nh√≥m d·ªØ li·ªáu theo **item ‚Üí store ‚Üí date** ƒë·ªÉ chu·∫©n b·ªã cho vi·ªác t·∫°o sequences.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Nh√≥m v√† s·∫Øp x·∫øp d·ªØ li·ªáu\ntrain_gp = train.sort_values('date').groupby(['item', 'store', 'date'], as_index=False)\ntrain_gp = train_gp.agg({'sales': ['mean']})\ntrain_gp.columns = ['item', 'store', 'date', 'sales']\n\nprint(f\"üìä D·ªØ li·ªáu sau khi t√°i c·∫•u tr√∫c: {train_gp.shape}\")\nprint(f\"   M·ªói d√≤ng = 1 (item, store, date) combination\")\ntrain_gp.head(10)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 5.3. Chuy·ªÉn ƒë·ªïi sang b√†i to√°n Supervised Learning\n\nƒê√¢y l√† b∆∞·ªõc quan tr·ªçng nh·∫•t trong ti·ªÅn x·ª≠ l√Ω. Ch√∫ng ta s·ª≠ d·ª•ng k·ªπ thu·∫≠t **Sliding Window** ƒë·ªÉ t·∫°o features t·ª´ c√°c timesteps tr∆∞·ªõc ƒë√≥.\n\n**C·∫•u h√¨nh:**\n- **Window size = 29:** S·ª≠ d·ª•ng 30 ng√†y g·∫ßn nh·∫•t (t-29 ƒë·∫øn t) l√†m input\n- **Lag = 90:** D·ª± b√°o doanh s·ªë 90 ng√†y trong t∆∞∆°ng lai\n\n```\nInput features: [sales(t-29), sales(t-28), ..., sales(t-1), sales(t)]\nTarget: sales(t+90)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n    \"\"\"\n    Chuy·ªÉn ƒë·ªïi time series th√†nh supervised learning format.\n    \n    Parameters:\n    -----------\n    data : DataFrame\n        D·ªØ li·ªáu g·ªëc\n    window : int\n        S·ªë timesteps tr∆∞·ªõc ƒë√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m features (lookback period)\n    lag : int  \n        S·ªë timesteps trong t∆∞∆°ng lai c·∫ßn d·ª± b√°o (forecast horizon)\n    dropnan : bool\n        C√≥ lo·∫°i b·ªè c√°c d√≤ng ch·ª©a NaN kh√¥ng\n        \n    Returns:\n    --------\n    DataFrame v·ªõi c√°c c·ªôt:\n        - Features: var(t-window), ..., var(t-1), var(t)\n        - Target: var(t+lag)\n    \"\"\"\n    cols, names = list(), list()\n    \n    # Input sequence: (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    \n    # Current timestep: (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    \n    # Target timestep: (t+lag)\n    cols.append(data.shift(-lag))\n    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n    \n    # Gh√©p t·∫•t c·∫£ l·∫°i\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    \n    # Lo·∫°i b·ªè NaN\n    if dropnan:\n        agg.dropna(inplace=True)\n    \n    return agg\n\nprint(\"‚úì ƒê·ªãnh nghƒ©a h√†m series_to_supervised()\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# √Åp d·ª•ng sliding window\nWINDOW_SIZE = 29  # S·ª≠ d·ª•ng 30 ng√†y (t-29 ƒë·∫øn t)\nFORECAST_HORIZON = lag_size  # 90 ng√†y\n\nprint(f\"‚öôÔ∏è C·∫§U H√åNH SLIDING WINDOW\")\nprint(f\"   - Window size: {WINDOW_SIZE} (s·ª≠ d·ª•ng {WINDOW_SIZE + 1} ng√†y l√†m features)\")\nprint(f\"   - Forecast horizon: {FORECAST_HORIZON} ng√†y\")\n\n# T·∫°o supervised learning dataset\nseries = series_to_supervised(\n    train_gp.drop('date', axis=1), \n    window=WINDOW_SIZE, \n    lag=FORECAST_HORIZON\n)\n\nprint(f\"\\nüìä K√≠ch th∆∞·ªõc sau chuy·ªÉn ƒë·ªïi: {series.shape}\")\nprint(f\"   - S·ªë samples: {series.shape[0]:,}\")\nprint(f\"   - S·ªë features: {series.shape[1]}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 5.4. X·ª≠ l√Ω d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá\n\nDo c√°ch t·∫°o sequences, m·ªôt s·ªë d√≤ng c√≥ th·ªÉ ch·ª©a d·ªØ li·ªáu t·ª´ c√°c item/store kh√°c nhau (khi sequence \"nh·∫£y\" t·ª´ item n√†y sang item kh√°c). C·∫ßn lo·∫°i b·ªè nh·ªØng d√≤ng n√†y.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Ki·ªÉm tra t√≠nh nh·∫•t qu√°n: item v√† store ph·∫£i gi·ªëng nhau trong to√†n b·ªô sequence\nlast_item = 'item(t-%d)' % WINDOW_SIZE\nlast_store = 'store(t-%d)' % WINDOW_SIZE\n\nbefore_filter = len(series)\n\n# Ch·ªâ gi·ªØ c√°c d√≤ng c√≥ c√πng store v√† item xuy√™n su·ªët sequence\nseries = series[(series['store(t)'] == series[last_store])]\nseries = series[(series['item(t)'] == series[last_item])]\n\nafter_filter = len(series)\n\nprint(f\"üîç L·ªåC D·ªÆ LI·ªÜU KH√îNG H·ª¢P L·ªÜ\")\nprint(f\"   Tr∆∞·ªõc: {before_filter:,} d√≤ng\")\nprint(f\"   Sau: {after_filter:,} d√≤ng\")\nprint(f\"   Lo·∫°i b·ªè: {before_filter - after_filter:,} d√≤ng ({(before_filter - after_filter)/before_filter*100:.2f}%)\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Lo·∫°i b·ªè c√°c c·ªôt item v√† store (kh√¥ng c·∫ßn cho training)\ncolumns_to_drop = [('%s(t+%d)' % (col, FORECAST_HORIZON)) for col in ['item', 'store']]\nfor i in range(WINDOW_SIZE, 0, -1):\n    columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['item', 'store']]\ncolumns_to_drop += ['item(t)', 'store(t)']\n\nseries.drop(columns_to_drop, axis=1, inplace=True)\n\nprint(f\"‚úì ƒê√£ lo·∫°i b·ªè c·ªôt item v√† store\")\nprint(f\"üìä K√≠ch th∆∞·ªõc cu·ªëi c√πng: {series.shape}\")\nprint(f\"\\nC√°c c·ªôt c√≤n l·∫°i (ƒë·∫ßu ti√™n v√† cu·ªëi c√πng):\")\nprint(f\"   Features: {series.columns[0]} ... {series.columns[-2]}\")\nprint(f\"   Target: {series.columns[-1]}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 5.5. Chia t·∫≠p Train/Validation\n\nChia d·ªØ li·ªáu th√†nh:\n- **Training set (60%):** D√πng ƒë·ªÉ train m√¥ h√¨nh\n- **Validation set (40%):** D√πng ƒë·ªÉ ƒë√°nh gi√° v√† tr√°nh overfitting",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# T√°ch features (X) v√† target (y)\nlabels_col = 'sales(t+%d)' % FORECAST_HORIZON\nlabels = series[labels_col]\nfeatures = series.drop(labels_col, axis=1)\n\n# Chia train/validation v·ªõi t·ªâ l·ªá 60/40\nX_train, X_valid, Y_train, Y_valid = train_test_split(\n    features, \n    labels.values, \n    test_size=0.4, \n    random_state=RANDOM_SEED\n)\n\nprint(\"üìä CHIA T·∫¨P D·ªÆ LI·ªÜU\")\nprint(\"=\" * 40)\nprint(f\"T·ªïng s·ªë samples: {len(features):,}\")\nprint(f\"\\nTraining set:\")\nprint(f\"   X_train: {X_train.shape}\")\nprint(f\"   Y_train: {Y_train.shape}\")\nprint(f\"\\nValidation set:\")\nprint(f\"   X_valid: {X_valid.shape}\")\nprint(f\"   Y_valid: {Y_valid.shape}\")\nprint(f\"\\nS·ªë features (timesteps): {X_train.shape[1]}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 6. X√ÇY D·ª∞NG M√î H√åNH\n\n### 6.1. C·∫•u h√¨nh chung\n\nC√°c hyperparameters ƒë∆∞·ª£c s·ª≠ d·ª•ng cho t·∫•t c·∫£ m√¥ h√¨nh:\n\n| Parameter | Gi√° tr·ªã | M√¥ t·∫£ |\n|-----------|---------|-------|\n| Epochs | 40 | S·ªë l·∫ßn l·∫∑p qua to√†n b·ªô training data |\n| Batch size | 256 | S·ªë samples trong m·ªói batch |\n| Learning rate | 0.0003 | T·ªëc ƒë·ªô h·ªçc c·ªßa optimizer |\n| Optimizer | Adam | Adaptive learning rate optimizer |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Hyperparameters\nEPOCHS = 40\nBATCH_SIZE = 256\nLEARNING_RATE = 0.0003\n\nprint(\"‚öôÔ∏è HYPERPARAMETERS\")\nprint(\"=\" * 40)\nprint(f\"Epochs: {EPOCHS}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"Optimizer: Adam\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### 6.2. M√¥ h√¨nh 1: Multi-Layer Perceptron (MLP)\n\n**Ki·∫øn tr√∫c:**\n```\nInput (30 features) ‚Üí Dense(100, ReLU) ‚Üí Dense(1) ‚Üí Output\n```\n\n**ƒê·∫∑c ƒëi·ªÉm:**\n- M·∫°ng neural ƒë∆°n gi·∫£n nh·∫•t\n- X·ª≠ l√Ω input nh∆∞ vector ph·∫≥ng, **kh√¥ng quan t√¢m th·ª© t·ª±** th·ªùi gian\n- Nhanh nh·∫•t ƒë·ªÉ train\n\n**Input shape:** `[samples, features]` = `[N, 30]`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Kh·ªüi t·∫°o optimizer m·ªõi cho m·ªói model\nadam = optimizers.Adam(learning_rate=LEARNING_RATE)\n\n# X√¢y d·ª±ng m√¥ h√¨nh MLP\nmodel_mlp = Sequential([\n    Dense(100, activation='relu', input_dim=X_train.shape[1], name='hidden_layer'),\n    Dense(1, name='output_layer')\n], name='MLP_Model')\n\nmodel_mlp.compile(loss='mse', optimizer=adam)\n\nprint(\"üß† M√î H√åNH MLP\")\nprint(\"=\" * 50)\nmodel_mlp.summary()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Training MLP\nprint(\"\\nüöÄ B·∫Øt ƒë·∫ßu training MLP...\")\nprint(\"-\" * 50)\n\nmlp_history = model_mlp.fit(\n    X_train.values, Y_train,\n    validation_data=(X_valid.values, Y_valid),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    verbose=1\n)\n\nprint(\"\\n‚úì Ho√†n th√†nh training MLP!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### 6.3. M√¥ h√¨nh 2: Convolutional Neural Network (CNN)\n\n**Ki·∫øn tr√∫c:**\n```\nInput (30, 1) ‚Üí Conv1D(64 filters, kernel=2) ‚Üí MaxPool(2) ‚Üí Flatten ‚Üí Dense(50, ReLU) ‚Üí Dense(1)\n```\n\n**ƒê·∫∑c ƒëi·ªÉm:**\n- S·ª≠ d·ª•ng **1D convolution** ƒë·ªÉ tr√≠ch xu·∫•t local patterns\n- Kernel size = 2: H·ªçc patterns t·ª´ 2 timesteps li√™n ti·∫øp\n- MaxPooling: Gi·∫£m chi·ªÅu v√† gi·ªØ l·∫°i features quan tr·ªçng\n\n**Input shape:** `[samples, timesteps, features]` = `[N, 30, 1]`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Reshape data cho CNN: th√™m chi·ªÅu features\nX_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n\nprint(\"üìê RESHAPE DATA CHO CNN/LSTM\")\nprint(f\"   X_train: {X_train.shape} ‚Üí {X_train_series.shape}\")\nprint(f\"   X_valid: {X_valid.shape} ‚Üí {X_valid_series.shape}\")\nprint(f\"\\n   Format: [samples, timesteps, features]\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Kh·ªüi t·∫°o optimizer m·ªõi\nadam = optimizers.Adam(learning_rate=LEARNING_RATE)\n\n# X√¢y d·ª±ng m√¥ h√¨nh CNN\nmodel_cnn = Sequential([\n    Conv1D(filters=64, kernel_size=2, activation='relu', \n           input_shape=(X_train_series.shape[1], X_train_series.shape[2]),\n           name='conv1d_layer'),\n    MaxPooling1D(pool_size=2, name='maxpool_layer'),\n    Flatten(name='flatten_layer'),\n    Dense(50, activation='relu', name='dense_layer'),\n    Dense(1, name='output_layer')\n], name='CNN_Model')\n\nmodel_cnn.compile(loss='mse', optimizer=adam)\n\nprint(\"üß† M√î H√åNH CNN\")\nprint(\"=\" * 50)\nmodel_cnn.summary()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Training CNN\nprint(\"\\nüöÄ B·∫Øt ƒë·∫ßu training CNN...\")\nprint(\"-\" * 50)\n\ncnn_history = model_cnn.fit(\n    X_train_series, Y_train,\n    validation_data=(X_valid_series, Y_valid),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    verbose=1\n)\n\nprint(\"\\n‚úì Ho√†n th√†nh training CNN!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### 6.4. M√¥ h√¨nh 3: Long Short-Term Memory (LSTM)\n\n**Ki·∫øn tr√∫c:**\n```\nInput (30, 1) ‚Üí LSTM(50 units, ReLU) ‚Üí Dense(1) ‚Üí Output\n```\n\n**ƒê·∫∑c ƒëi·ªÉm:**\n- X·ª≠ l√Ω input nh∆∞ **sequence c√≥ th·ª© t·ª±**\n- LSTM cells c√≥ kh·∫£ nƒÉng **nh·ªõ th√¥ng tin d√†i h·∫°n** th√¥ng qua:\n  - Forget gate: Quy·∫øt ƒë·ªãnh th√¥ng tin n√†o c·∫ßn qu√™n\n  - Input gate: Quy·∫øt ƒë·ªãnh th√¥ng tin m·ªõi n√†o c·∫ßn nh·ªõ\n  - Output gate: Quy·∫øt ƒë·ªãnh output d·ª±a tr√™n cell state\n- Ph√π h·ª£p cho time series c√≥ **long-term dependencies**\n\n**Input shape:** `[samples, timesteps, features]` = `[N, 30, 1]`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Kh·ªüi t·∫°o optimizer m·ªõi\nadam = optimizers.Adam(learning_rate=LEARNING_RATE)\n\n# X√¢y d·ª±ng m√¥ h√¨nh LSTM\nmodel_lstm = Sequential([\n    LSTM(50, activation='relu', \n         input_shape=(X_train_series.shape[1], X_train_series.shape[2]),\n         name='lstm_layer'),\n    Dense(1, name='output_layer')\n], name='LSTM_Model')\n\nmodel_lstm.compile(loss='mse', optimizer=adam)\n\nprint(\"üß† M√î H√åNH LSTM\")\nprint(\"=\" * 50)\nmodel_lstm.summary()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Training LSTM\nprint(\"\\nüöÄ B·∫Øt ƒë·∫ßu training LSTM...\")\nprint(\"-\" * 50)\nprint(\"‚ö†Ô∏è  LSTM training ch·∫≠m h∆°n c√°c m√¥ h√¨nh kh√°c, vui l√≤ng ch·ªù...\")\n\nlstm_history = model_lstm.fit(\n    X_train_series, Y_train,\n    validation_data=(X_valid_series, Y_valid),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    verbose=1\n)\n\nprint(\"\\n‚úì Ho√†n th√†nh training LSTM!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### 6.5. M√¥ h√¨nh 4: Hybrid CNN-LSTM\n\n**Ki·∫øn tr√∫c:**\n```\nInput (2, 15, 1) ‚Üí TimeDistributed(Conv1D) ‚Üí TimeDistributed(MaxPool) ‚Üí TimeDistributed(Flatten) ‚Üí LSTM(50) ‚Üí Dense(1)\n```\n\n**√ù t∆∞·ªüng:**\n- Chia sequence th√†nh **2 subsequences**, m·ªói subsequence 15 timesteps\n- **CNN:** Tr√≠ch xu·∫•t local features t·ª´ m·ªói subsequence\n- **LSTM:** H·ªçc temporal relationships gi·ªØa c√°c subsequences\n\n**∆Øu ƒëi·ªÉm:**\n- K·∫øt h·ª£p kh·∫£ nƒÉng feature extraction c·ªßa CNN v√† sequence modeling c·ªßa LSTM\n- C√≥ th·ªÉ x·ª≠ l√Ω sequences r·∫•t d√†i b·∫±ng c√°ch chia nh·ªè\n\n**Input shape:** `[samples, subsequences, timesteps, features]` = `[N, 2, 15, 1]`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Reshape data: chia m·ªói sequence th√†nh 2 subsequences\nN_SUBSEQUENCES = 2\nSUBSEQ_TIMESTEPS = X_train_series.shape[1] // N_SUBSEQUENCES\n\nX_train_series_sub = X_train_series.reshape(\n    (X_train_series.shape[0], N_SUBSEQUENCES, SUBSEQ_TIMESTEPS, 1)\n)\nX_valid_series_sub = X_valid_series.reshape(\n    (X_valid_series.shape[0], N_SUBSEQUENCES, SUBSEQ_TIMESTEPS, 1)\n)\n\nprint(\"üìê RESHAPE DATA CHO CNN-LSTM\")\nprint(f\"   X_train: {X_train_series.shape} ‚Üí {X_train_series_sub.shape}\")\nprint(f\"   X_valid: {X_valid_series.shape} ‚Üí {X_valid_series_sub.shape}\")\nprint(f\"\\n   Format: [samples, subsequences, timesteps, features]\")\nprint(f\"   M·ªói sample ƒë∆∞·ª£c chia th√†nh {N_SUBSEQUENCES} subsequences\")\nprint(f\"   M·ªói subsequence c√≥ {SUBSEQ_TIMESTEPS} timesteps\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Kh·ªüi t·∫°o optimizer m·ªõi\nadam = optimizers.Adam(learning_rate=LEARNING_RATE)\n\n# X√¢y d·ª±ng m√¥ h√¨nh CNN-LSTM\nmodel_cnn_lstm = Sequential([\n    # CNN layers wrapped in TimeDistributed ƒë·ªÉ apply cho t·ª´ng subsequence\n    TimeDistributed(\n        Conv1D(filters=64, kernel_size=1, activation='relu'),\n        input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3]),\n        name='td_conv1d'\n    ),\n    TimeDistributed(MaxPooling1D(pool_size=2), name='td_maxpool'),\n    TimeDistributed(Flatten(), name='td_flatten'),\n    \n    # LSTM layer ƒë·ªÉ h·ªçc relationships gi·ªØa c√°c subsequences\n    LSTM(50, activation='relu', name='lstm_layer'),\n    \n    # Output layer\n    Dense(1, name='output_layer')\n], name='CNN_LSTM_Model')\n\nmodel_cnn_lstm.compile(loss='mse', optimizer=adam)\n\nprint(\"üß† M√î H√åNH CNN-LSTM\")\nprint(\"=\" * 50)\nmodel_cnn_lstm.summary()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Training CNN-LSTM\nprint(\"\\nüöÄ B·∫Øt ƒë·∫ßu training CNN-LSTM...\")\nprint(\"-\" * 50)\nprint(\"‚ö†Ô∏è  ƒê√¢y l√† m√¥ h√¨nh ph·ª©c t·∫°p nh·∫•t, c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian...\")\n\ncnn_lstm_history = model_cnn_lstm.fit(\n    X_train_series_sub, Y_train,\n    validation_data=(X_valid_series_sub, Y_valid),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    verbose=1\n)\n\nprint(\"\\n‚úì Ho√†n th√†nh training CNN-LSTM!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 7. ƒê√ÅNH GI√Å V√Ä SO S√ÅNH M√î H√åNH\n\n### 7.1. Tr·ª±c quan h√≥a Learning Curves\n\nLearning curves cho th·∫•y qu√° tr√¨nh training c·ªßa m√¥ h√¨nh qua c√°c epochs. Ch√∫ng ta c√≥ th·ªÉ nh·∫≠n bi·∫øt:\n- **Overfitting:** Train loss gi·∫£m nh∆∞ng validation loss tƒÉng\n- **Underfitting:** C·∫£ hai loss ƒë·ªÅu cao\n- **Good fit:** C·∫£ hai loss ƒë·ªÅu gi·∫£m v√† h·ªôi t·ª•",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# V·∫Ω learning curves cho t·∫•t c·∫£ m√¥ h√¨nh\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\nfig.suptitle('LEARNING CURVES - SO S√ÅNH C√ÅC M√î H√åNH', fontsize=16, fontweight='bold')\n\n# MLP\nax1 = axes[0, 0]\nax1.plot(mlp_history.history['loss'], label='Train Loss', color='#2E86AB', linewidth=2)\nax1.plot(mlp_history.history['val_loss'], label='Validation Loss', color='#E63946', linewidth=2)\nax1.set_title('MLP (Multi-Layer Perceptron)', fontsize=12, fontweight='bold')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('MSE Loss')\nax1.legend(loc='upper right')\nax1.grid(True, alpha=0.3)\n\n# CNN\nax2 = axes[0, 1]\nax2.plot(cnn_history.history['loss'], label='Train Loss', color='#2E86AB', linewidth=2)\nax2.plot(cnn_history.history['val_loss'], label='Validation Loss', color='#E63946', linewidth=2)\nax2.set_title('CNN (Convolutional Neural Network)', fontsize=12, fontweight='bold')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('MSE Loss')\nax2.legend(loc='upper right')\nax2.grid(True, alpha=0.3)\n\n# LSTM\nax3 = axes[1, 0]\nax3.plot(lstm_history.history['loss'], label='Train Loss', color='#2E86AB', linewidth=2)\nax3.plot(lstm_history.history['val_loss'], label='Validation Loss', color='#E63946', linewidth=2)\nax3.set_title('LSTM (Long Short-Term Memory)', fontsize=12, fontweight='bold')\nax3.set_xlabel('Epoch')\nax3.set_ylabel('MSE Loss')\nax3.legend(loc='upper right')\nax3.grid(True, alpha=0.3)\n\n# CNN-LSTM\nax4 = axes[1, 1]\nax4.plot(cnn_lstm_history.history['loss'], label='Train Loss', color='#2E86AB', linewidth=2)\nax4.plot(cnn_lstm_history.history['val_loss'], label='Validation Loss', color='#E63946', linewidth=2)\nax4.set_title('CNN-LSTM (Hybrid Model)', fontsize=12, fontweight='bold')\nax4.set_xlabel('Epoch')\nax4.set_ylabel('MSE Loss')\nax4.legend(loc='upper right')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2. T√≠nh to√°n RMSE cho t·ª´ng m√¥ h√¨nh\n\n**Root Mean Squared Error (RMSE)** ƒëo l∆∞·ªùng sai s·ªë trung b√¨nh gi·ªØa gi√° tr·ªã th·ª±c v√† gi√° tr·ªã d·ª± ƒëo√°n. RMSE c√†ng th·∫•p ‚Üí m√¥ h√¨nh c√†ng ch√≠nh x√°c.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Dictionary ƒë·ªÉ l∆∞u k·∫øt qu·∫£\nresults = {}\n\nprint(\"=\"*60)\nprint(\"ƒê√ÅNH GI√Å M√î H√åNH - RMSE (Root Mean Squared Error)\")\nprint(\"=\"*60)\n\n# MLP\nmlp_train_pred = model_mlp.predict(X_train.values, verbose=0)\nmlp_valid_pred = model_mlp.predict(X_valid.values, verbose=0)\nmlp_train_rmse = np.sqrt(mean_squared_error(Y_train, mlp_train_pred))\nmlp_valid_rmse = np.sqrt(mean_squared_error(Y_valid, mlp_valid_pred))\nresults['MLP'] = {'train': mlp_train_rmse, 'valid': mlp_valid_rmse}\n\nprint(f\"\\nüìä MLP:\")\nprint(f\"   Train RMSE:      {mlp_train_rmse:.4f}\")\nprint(f\"   Validation RMSE: {mlp_valid_rmse:.4f}\")\n\n# CNN\ncnn_train_pred = model_cnn.predict(X_train_series, verbose=0)\ncnn_valid_pred = model_cnn.predict(X_valid_series, verbose=0)\ncnn_train_rmse = np.sqrt(mean_squared_error(Y_train, cnn_train_pred))\ncnn_valid_rmse = np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred))\nresults['CNN'] = {'train': cnn_train_rmse, 'valid': cnn_valid_rmse}\n\nprint(f\"\\nüìä CNN:\")\nprint(f\"   Train RMSE:      {cnn_train_rmse:.4f}\")\nprint(f\"   Validation RMSE: {cnn_valid_rmse:.4f}\")\n\n# LSTM\nlstm_train_pred = model_lstm.predict(X_train_series, verbose=0)\nlstm_valid_pred = model_lstm.predict(X_valid_series, verbose=0)\nlstm_train_rmse = np.sqrt(mean_squared_error(Y_train, lstm_train_pred))\nlstm_valid_rmse = np.sqrt(mean_squared_error(Y_valid, lstm_valid_pred))\nresults['LSTM'] = {'train': lstm_train_rmse, 'valid': lstm_valid_rmse}\n\nprint(f\"\\nüìä LSTM:\")\nprint(f\"   Train RMSE:      {lstm_train_rmse:.4f}\")\nprint(f\"   Validation RMSE: {lstm_valid_rmse:.4f}\")\n\n# CNN-LSTM\ncnn_lstm_train_pred = model_cnn_lstm.predict(X_train_series_sub, verbose=0)\ncnn_lstm_valid_pred = model_cnn_lstm.predict(X_valid_series_sub, verbose=0)\ncnn_lstm_train_rmse = np.sqrt(mean_squared_error(Y_train, cnn_lstm_train_pred))\ncnn_lstm_valid_rmse = np.sqrt(mean_squared_error(Y_valid, cnn_lstm_valid_pred))\nresults['CNN-LSTM'] = {'train': cnn_lstm_train_rmse, 'valid': cnn_lstm_valid_rmse}\n\nprint(f\"\\nüìä CNN-LSTM:\")\nprint(f\"   Train RMSE:      {cnn_lstm_train_rmse:.4f}\")\nprint(f\"   Validation RMSE: {cnn_lstm_valid_rmse:.4f}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 7.3. B·∫£ng t·ªïng h·ª£p k·∫øt qu·∫£",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# T·∫°o DataFrame t·ªïng h·ª£p\nresults_df = pd.DataFrame({\n    'M√¥ h√¨nh': ['MLP', 'CNN', 'LSTM', 'CNN-LSTM'],\n    'Train RMSE': [results['MLP']['train'], results['CNN']['train'], \n                   results['LSTM']['train'], results['CNN-LSTM']['train']],\n    'Validation RMSE': [results['MLP']['valid'], results['CNN']['valid'],\n                        results['LSTM']['valid'], results['CNN-LSTM']['valid']]\n})\n\n# T√≠nh ƒë·ªô ch√™nh l·ªách (gap) gi·ªØa train v√† validation\nresults_df['Gap (Val - Train)'] = results_df['Validation RMSE'] - results_df['Train RMSE']\n\n# X·∫øp h·∫°ng theo Validation RMSE\nresults_df['X·∫øp h·∫°ng'] = results_df['Validation RMSE'].rank().astype(int)\nresults_df = results_df.sort_values('X·∫øp h·∫°ng')\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"B·∫¢NG T·ªîNG H·ª¢P K·∫æT QU·∫¢\")\nprint(\"=\"*70)\nprint(results_df.to_string(index=False))\nprint(\"\\nüìå Gap d∆∞∆°ng l·ªõn c√≥ th·ªÉ ch·ªâ ra overfitting\")\nprint(\"üìå Validation RMSE l√† metric quan tr·ªçng nh·∫•t ƒë·ªÉ ƒë√°nh gi√° generalization\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Bi·ªÉu ƒë·ªì so s√°nh RMSE\nfig, ax = plt.subplots(figsize=(12, 6))\n\nx = np.arange(len(results_df))\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, results_df['Train RMSE'], width, \n               label='Train RMSE', color='#2E86AB', alpha=0.8)\nbars2 = ax.bar(x + width/2, results_df['Validation RMSE'], width,\n               label='Validation RMSE', color='#E63946', alpha=0.8)\n\nax.set_xlabel('M√¥ h√¨nh', fontsize=12)\nax.set_ylabel('RMSE', fontsize=12)\nax.set_title('SO S√ÅNH HI·ªÜU SU·∫§T C√ÅC M√î H√åNH', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(results_df['M√¥ h√¨nh'])\nax.legend()\nax.grid(axis='y', alpha=0.3)\n\n# Th√™m gi√° tr·ªã l√™n bars\nfor bar in bars1:\n    height = bar.get_height()\n    ax.annotate(f'{height:.2f}',\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3), textcoords=\"offset points\",\n                ha='center', va='bottom', fontsize=9)\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.annotate(f'{height:.2f}',\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3), textcoords=\"offset points\",\n                ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. K·∫æT LU·∫¨N\n\n### 8.1. T·ªïng k·∫øt k·∫øt qu·∫£\n\nQua qu√° tr√¨nh th·ª±c nghi·ªám v·ªõi 4 m√¥ h√¨nh Deep Learning cho b√†i to√°n d·ª± b√°o nhu c·∫ßu, ch√∫ng ta c√≥ th·ªÉ r√∫t ra c√°c k·∫øt lu·∫≠n sau:\n\n**V·ªÅ hi·ªáu su·∫•t d·ª± b√°o:**\n- C√°c m√¥ h√¨nh ƒë·ªÅu cho k·∫øt qu·∫£ t∆∞∆°ng ƒë·ªëi t·ªët v·ªõi RMSE trong kho·∫£ng h·ª£p l√Ω\n- [ƒêi·ªÅn nh·∫≠n x√©t c·ª• th·ªÉ d·ª±a tr√™n k·∫øt qu·∫£ th·ª±c t·∫ø]\n\n**V·ªÅ ƒë·∫∑c ƒëi·ªÉm t·ª´ng m√¥ h√¨nh:**\n\n| M√¥ h√¨nh | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |\n|---------|---------|------------|\n| MLP | ƒê∆°n gi·∫£n, train nhanh | Kh√¥ng capture sequential patterns |\n| CNN | Ph√°t hi·ªán local patterns t·ªët | Kh√≥ h·ªçc long-range dependencies |\n| LSTM | X·ª≠ l√Ω sequences t·ªët | Train ch·∫≠m, nhi·ªÅu parameters |\n| CNN-LSTM | K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·∫£ hai | Ph·ª©c t·∫°p, c·∫ßn tune nhi·ªÅu |\n\n### 8.2. H·∫°n ch·∫ø c·ªßa nghi√™n c·ª©u\n\n1. **D·ªØ li·ªáu:** Ch·ªâ s·ª≠ d·ª•ng 1 nƒÉm data (2017) ƒë·ªÉ gi·∫£m th·ªùi gian training\n2. **Features:** Ch·ªâ s·ª≠ d·ª•ng historical sales, ch∆∞a th√™m external features (holidays, promotions, weather...)\n3. **Hyperparameter tuning:** Ch∆∞a th·ª±c hi·ªán grid search ho·∫∑c random search\n4. **Ensemble:** Ch∆∞a th·ª≠ k·∫øt h·ª£p nhi·ªÅu m√¥ h√¨nh\n\n### 8.3. H∆∞·ªõng ph√°t tri·ªÉn\n\n1. **Feature Engineering:**\n   - Th√™m calendar features (day of week, month, holiday)\n   - Th√™m lag features v·ªõi c√°c windows kh√°c nhau\n   - Th√™m rolling statistics (moving average, std)\n\n2. **M√¥ h√¨nh n√¢ng cao:**\n   - Transformer-based models (Temporal Fusion Transformer)\n   - N-BEATS, DeepAR\n   - Ensemble methods\n\n3. **C·∫£i thi·ªán training:**\n   - Early stopping\n   - Learning rate scheduling\n   - Cross-validation cho time series\n\n---\n\n## T√ÄI LI·ªÜU THAM KH·∫¢O\n\n1. Brownlee, J. (2018). *Deep Learning for Time Series Forecasting*. Machine Learning Mastery.\n2. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural computation*, 9(8), 1735-1780.\n3. Kaggle. Store Item Demand Forecasting Challenge. https://www.kaggle.com/c/demand-forecasting-kernels-only\n4. TensorFlow Documentation. https://www.tensorflow.org/tutorials/structured_data/time_series\n\n---\n\n**¬© 2024 - ƒê·ªì √°n m√¥n Deep Learning - UIT**",
   "metadata": {}
  }
 ]
}
